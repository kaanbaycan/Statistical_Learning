\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Chapter 3 Exercises}
\author{Yakup Kaan Baycan}
\date{July 2023}

\begin{document}

\maketitle

\section{3.7 Statistical Learning Exercises}
\subsection{Conceptual}

\textbf{1 - Describe the null hypotheses to which the p-values given in Table 3.4
correspond. Explain what conclusions you can draw based on these
p-values. Your explanation should be phrased in terms of sales, TV,
radio, and newspaper, rather than in terms of the coefficients of the
linear model.} \\ 

The table is created according to the multiple regression model using the variables: TV, radio, and newspaper. P-values respectively are: <0.0001,<0.0001, 0.8599. These p-values represent the importance of the variables. Usually, p-values that are less than 0.05 count as significant. Hence the null hypothesis is:\\
$H_0$: $\beta_i$ $\neq0  \forall i \in (0,1,2,3).$\\
We use a t-test in order to obtain a p-value for the training data. As a result, we say that sales are highly affected by TV and radio in a positive correlation. But, with this high p-value newspaper seems to not affect the sales as much as other variables.

\textbf{2 - Carefully explain the differences between the KNN classifier and KNN
regression methods.}\\

The key difference between two algorithms is that KNN classifier tries to class the response variable into a discrete set while KNN regression method tries to predict the response variable. KNN classifier method works with the nearest n neighbor and predicts the class due. On the other hand, KNN regression method uses the nearest n neighbor's mean to predict the value. 

\textbf{3 - Suppose we have a data set with five predictors, X1 = GPA, X2 =
IQ, X3 = Level (1 for College and 0 for High School), X4 = Interaction
between GPA and IQ, and X5 = Interaction between GPA and
Level. The response is starting salary after graduation (in thousands
of dollars). Suppose we use least squares to fit the model, and get
$\hat{\beta_0} = 50$, $\hat{\beta_1} = 20$, $\hat{\beta_2} = 0.07$, $\hat{\beta_3} = 35$, $\hat{\beta_4} = 0.01$, $\hat{\beta_5} = −10$.} \\

\textbf{a)Which answer is correct, and why?}\\

iv. For a fixed value of IQ and GPA, college graduates earn
more, on average, than high school graduates provided that
the GPA is high enough. This is because of the last term has a negative effect on the college degree case.
\textbf{b)Predict the salary of a college graduate with IQ of 110 and a
GPA of 4.0.}\\

$\hat{Salary} = 50 + 20 \times 4 + 0.07 \times 110 + 35 \times 1 + 0.01 \times(110\times4) - 10 \times (1\times4) = 137.1k$\\

\textbf{c)True or false: Since the coefficient for the GPA/IQ interaction
term is very small, there is very little evidence of an interaction
effect. Justify your answer.}\\

False, in order to make a comment on the significance of the interaction we must check the p-value and t-stat. Here, the coefficient may be small but the interaction already includes two variables multiplied which may affect the response variable quite significantly even though the coefficient is small.\\

\textbf{4 - I collect a set of data (n = 100 observations) containing a single
predictor and a quantitative response. I then fit a linear regression
model to the data, as well as a separate cubic regression, i.e. Y =
$\beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$.}\\

\textbf{a) Suppose that the true relationship between X and Y is linear,
i.e. $Y = \beta_0 + \beta_1X + \epsilon$. Consider the training residual sum of
squares (RSS) for the linear regression, and also the training
RSS for the cubic regression. Would we expect one to be lower
than the other, would we expect them to be the same, or is there
not enough information to tell? Justify your answer.}\\

The cubic model is a more flexible model reducing the bias but with high variance. On the other hand, the linear model is less flexible but making more assumptions about the true form of f. Hence, the true form of f is given as linear so using a cubic model will end up with high variance on the test data. So RSS of the cubic model on the training data may be lower than the linear model, but we must be aware that this happened with overfitting. As a result, lower RSS is not the only metric we should check.\\

\textbf{b) Answer (a) using test rather than training RSS.}\\

Just as I mentioned, the cubic model overfits in the training data reducing bias with high variance. So in the test dataset, the cubic model will underperform contrary to linear model.\\

\textbf{(c) Suppose that the true relationship between X and Y is not linear,
but we don’t know how far it is from linear. Consider the training
RSS for the linear regression, and also the training RSS for the
cubic regression. Would we expect one to be lower than the
other, would we expect them to be the same, or is there not
enough information to tell? Justify your answer.}\\

As mentioned above, more flexible methods result in lower RSS\\

\textbf{(d) Answer (c) using test rather than training RSS.}\\

In the test data, a bias-variance trade-off comes in and most probably, the linear model will perform better in the test data but without further information about the true form of f we cannot say much.\\

\textbf{5 - Consider the fitted values that result from performing linear regression without an intercept. In this setting, the i-th fitted value takes the form $\hat{y_i} = x_i\hat{\beta}$,
where:\\
$\beta = (\sum(x_iy_i)/\sum(x_i^2))$ \\Show that we can write $\hat{y_i} = \sum(a_iy_i)$. What is $a_i$?}\\

$\hat{y_i} = x_i \sum(x_iy_i)/\sum(x_i^2)$\\

$\hat{y_i} = x_i \sum(y_i)/\sum(x_i)$\\

$a_i = x_i / \sum(x_i)$ \\

\textbf{6 - Using (3.4), argue that in the case of simple linear regression, the
least squares line always passes through the point ($\bar{x},\bar{y}$).}\\

If we write the line equation:\\

$\hat{y} = \hat{\beta_0} + \hat{\beta_1x_1}$\\

$RSS = (y-\hat{y})^2$

We try to minimize RSS, $RSS = (y- \hat{\beta_0} + \hat{\beta_1x_1}$


\end{document}
